# 吴恩达机器学习笔记（2）——多变量线性回归

> 上一篇我们提到了单变量的线性回归模型，但是我们实际遇到的问题，都会有多个变量影响，比如上篇的例子——房价问题，在实际情况下影响房价的一定不止房子的面积，房子的地理位置，采光度等等都会或多或少的影响房价，所以必须考虑更多变量来使我们的预测模型更加精确，这里就教大家多变量的线性回归模型。


### 例题
我们这次的例题还是用我们上次单变量线性回归模型一样的问题——房价问题，但是我们这次添加了房间数，房子所在楼层数，房子面积这三个新变量到我们的例题中。

![](https://upload-images.jianshu.io/upload_images/8355793-b86f08152156557e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


我们把这四个变量命名为x1,x2,x3,x4结果为y，用n表示我们有n个特征，用m表示样本数量。既然我们多了这么多特征变量，那么我们的模型的公式也要改变：

![新模型公式](https://upload-images.jianshu.io/upload_images/8355793-efd49ea1929f17e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但为了计算和表示的方便我们多设一个变量x0=1，也就是说我们的θ0 * x0，这样我们可以把特征表示成一个矩阵`x = [x0,x1,x2........,xn]`，同时我们的要求的参数θ也能表示成同长度的矩阵`θ = [θ0,θ1,θ2.....,θ3]`。

![](https://upload-images.jianshu.io/upload_images/8355793-2b4378106c1f25d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样我们的h(x)就可以表示成h(x) =(θ^T )*x

![](https://upload-images.jianshu.io/upload_images/8355793-81825ec92ebe369e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 梯度下降

多元线性回归的代价函数和单变量的回归的代价函数是一样的，所以我们这里的梯度下降的方法和公式没有太大的差别，都是原参数减去学习率与原参数在代价函数上的偏导的乘积：

![](https://upload-images.jianshu.io/upload_images/8355793-5508b1745b5f6073.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 特征缩放
虽然多元线性回归的梯度下降方法和单变量回归没有太大的差别，但是多元线性回归涉及到单变量回归没有的问题，就是多个特征值的取值差别过大，导致模型很难拟合。这里我们需要运用特征缩放的方法对数据进行预处理。特征缩放其实很简单，比如我们的两个变量房屋的面积和房间数。房屋的面积 的范围在0-2000英寸，而我们的房间数在1-5间，这两个特征值就相差过大，导致梯度下降的困难。这里我们只需要把我们的房屋面积除以2000，房间数除以5，这样我们两个特征值的范围就在0-1之间，梯度下降就更加容易。

![](https://upload-images.jianshu.io/upload_images/8355793-1fe217e5e874349b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 学习率选择

这里给的是学习率选择的技巧。
首先我们可以绘制出梯度下降迭代的次数和代价函数值的关系图，来判断函数是否收敛，从而判断这个学习率是否合适。

![](https://upload-images.jianshu.io/upload_images/8355793-368be50652526c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假如我们的图像函数曲线随着迭代次数增加而上升，说明我们的学习率过大，如果我们的曲线是忽上忽下，则说明我们的学习率偏小。根据图像我们就可以找到方向修改我们的学习率到合适的大小。

![](https://upload-images.jianshu.io/upload_images/8355793-5407f206ad07527b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们推荐学习率的改变方式是每十倍取值，比如0.001偏小了我们下次就取0.01，再偏小再取0.1。

### 特征与多项式回归

很多时候我们的数据分布不是直线型的，可能是曲线，也可能是其他图形，这里将教大家如何拟合非直线型函数。比如这个房屋面积与价格的数据，我们可以看出来像一条抛物线，我们就可以用二次模型去拟合。如果后面继续上升了，就像一个三次函数，我们就用三次模型去拟合，然后把每一项替换了。

![](https://upload-images.jianshu.io/upload_images/8355793-3ba78f8aec7be5f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个问题就转换成了用x1，x2，x3来建立回归。让我们的模型拟合度更高。但是要记住不要忘了特征缩放。毕竟三次和一次的值会相差很大，增加梯度下降的难度。当然选择什么函数模型去拟合，还是取决于数据在坐标系上的分布像什么函数模型。

### 正规方程

其实很多读者在我们给出例题的时候就想到了我们高中时候学过的最小二成法求回归方程，就会产生疑惑，我们明明可以直接求取各个参数，为什么还要用梯度下降这种费时费力的方式来求回归方程呢？这里就给出了直接求取参数的正规方程。我们梯度下降的目的是求出代价函数的取得最小值或者局部最小值的值，在我们高中就学过用导数等于零来求极大值和极小值，这里我们就是用代价函数导数等于零来求参数的解。我门就是把逐个参数θ求代价函数J的偏导，然后把他们全部置零，并求解。

![](https://upload-images.jianshu.io/upload_images/8355793-e21643ba8f73e24f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但这样求解的方式很麻烦，所以这里给出一般解法，首先我们构建x0，x1，x3.....xn的所以特征值的一个矩阵，以及所有目标值y的矩阵，这里我们给出一个例子：

![](https://upload-images.jianshu.io/upload_images/8355793-3174baff9924383a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么我们的参数的解也是一个矩阵，可以通过这个公式得到：

![正规方程](https://upload-images.jianshu.io/upload_images/8355793-e55535b8969f2ca1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

使用这种方法我们就可以不用对特征值进行缩放了
### 正规方程法与梯度下降法的比较
上面给出的正规方程解法，一步我们就可以得到我们想要的结果，是不是很方便，但是正规方程真的这么万能的话我们也不会在前面废这么大的劲来讲解梯度下降法。这里我们就给出两种方法的优劣。
首先梯度下降我们要确定学习率，而且需要很多次迭代来训练，费时费力，正规方程则不需要。
但是我们特征过多的话，正规方程求解的时间维度是O（n ^ 3），这就需要比梯度下降更多的时间来求解。梯度下降适用于更多的情况，而且符合机器学习的方法，所以我们在做机器学习更倾向于用梯度下降而不是用正规方程。

### 后记
写完这篇我大概就要准备期末考试了，大概更新的速度会很慢很慢，可能两个星期一更吧。。。祝我考试顺利吧。